{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy import io\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import test_train_split, evaluate_model_torch, subtract_spont, corrcoef, PCA,zscore\n",
    "\n",
    "class EnsemblePursuitNumpy():\n",
    "    def __init__(self,n_ensembles,lambd,options_dict):\n",
    "        self.n_ensembles=n_ensembles\n",
    "        self.lambd=lambd\n",
    "        self.options_dict=options_dict\n",
    "\n",
    "    def zscore(self,X):\n",
    "        mean_stimuli=np.mean(X.T,axis=0)\n",
    "        std_stimuli=np.std(X.T,axis=0,ddof=1)+1e-10\n",
    "        X=np.subtract(X.T,mean_stimuli)\n",
    "        X=np.divide(X,std_stimuli)\n",
    "        return X.T\n",
    "\n",
    "    def calculate_dot_squared(self,C_summed):\n",
    "        dot_squared=np.clip(C_summed,a_min=0,a_max=None)**2\n",
    "        return dot_squared\n",
    "\n",
    "    def calculate_cost_delta(self,C_summed,current_v):\n",
    "        cost_delta=np.clip(C_summed,a_min=0,a_max=None)**2/(self.sz[1]*(current_v**2).sum())-self.lambd\n",
    "        return cost_delta\n",
    "\n",
    "    def mask_dot_squared(self,selected_neurons,dot_squared):\n",
    "        mask=np.zeros((selected_neurons.shape[0]),dtype=bool)\n",
    "        mask[selected_neurons==0]=1\n",
    "        mask[selected_neurons!=0]=0\n",
    "        masked_dot_squared=mask*dot_squared\n",
    "        return masked_dot_squared\n",
    "\n",
    "    def sum_C(self,C_summed_unnorm,C,max_delta_neuron):\n",
    "        C_summed_unnorm=C_summed_unnorm+C[:,max_delta_neuron]\n",
    "        return C_summed_unnorm\n",
    "\n",
    "    def sum_v(self, v, max_delta_neuron, X):\n",
    "        current_v=v+X[max_delta_neuron,:]\n",
    "        return current_v\n",
    "\n",
    "    def fit_one_ensemble(self,X,C):\n",
    "        #A parameter to account for how many top neurons we sample from. It starts from 1,\n",
    "        #because we choose the top neuron when possible, e.g. when we can find an ensemble\n",
    "        # that is larger than min ensemble size. If there is no ensemble with the top neuron\n",
    "        # we increase the number of neurons to sample from.\n",
    "        self.n_neurons_for_sampling=1\n",
    "        n=0\n",
    "        min_assembly_size=self.options_dict['min_assembly_size']\n",
    "        #index for switching between top neurons for fitting ensemble when the first neurons\n",
    "        #doesn't give large enough ensemble\n",
    "        index=-1\n",
    "        #A while loop for trying sampling other neurons if the found ensemble size is smaller\n",
    "        #than threshold.\n",
    "        while n<min_assembly_size:\n",
    "            start=time.time()\n",
    "            seed=self.repeated_seed(C,index)\n",
    "            end=time.time()\n",
    "            print('old seed', end-start,seed)\n",
    "            seed=self.fast_sort(C,index)\n",
    "            end2=time.time()\n",
    "            print('new seed',end2-end,seed)\n",
    "            n=1\n",
    "            current_v=X[seed,:]\n",
    "            current_v_unnorm=current_v.copy()\n",
    "            selected_neurons=np.zeros((X.shape[0]),dtype=bool)\n",
    "            #Seed current_v\n",
    "            selected_neurons[seed]=1\n",
    "            #Fake cost to initiate while loop\n",
    "            max_cost_delta=1000\n",
    "            C_summed_unnorm=0\n",
    "            max_delta_neuron=seed\n",
    "            while max_cost_delta>0:\n",
    "                #Add the x corresponding to the max delta neuron to C_sum. Saves computational\n",
    "                #time.\n",
    "                #print(n)\n",
    "                C_summed_unnorm=self.sum_C(C_summed_unnorm,C,max_delta_neuron)\n",
    "                C_summed=(1./n)*C_summed_unnorm\n",
    "                dot_squared=self.calculate_dot_squared(C_summed)\n",
    "                #invert the 0's and 1's in the array which stores which neurons have already\n",
    "                #been selected into the assembly to use it as a mask\n",
    "                masked_dot_squared=self.mask_dot_squared(selected_neurons,dot_squared)\n",
    "                max_delta_neuron=np.argmax(masked_dot_squared)\n",
    "                cost_delta=self.calculate_cost_delta(C_summed[max_delta_neuron],current_v)\n",
    "                #print(cost_delta)\n",
    "                if cost_delta>0:\n",
    "                    selected_neurons[max_delta_neuron]=1\n",
    "                    current_v_unnorm= self.sum_v(current_v_unnorm,max_delta_neuron,X)\n",
    "                    n+=1\n",
    "                    current_v=(1./n)*current_v_unnorm\n",
    "                max_cost_delta=cost_delta\n",
    "\n",
    "            index+=-1\n",
    "        print('nr of neurons in ensemble',n)\n",
    "        current_u=np.zeros((X.shape[0],1))\n",
    "        current_u[selected_neurons,0]=np.clip(C_summed[selected_neurons],a_min=0,a_max=None)/(current_v**2).sum()\n",
    "        self.U=np.concatenate((self.U,current_u),axis=1)\n",
    "        self.V=np.concatenate((self.V,current_v.reshape(1,self.sz[1])),axis=0)\n",
    "        return current_u, current_v, C, selected_neurons\n",
    "               \n",
    "        \n",
    "    def fit_one_ensemble_(self,X,C):\n",
    "        #A parameter to account for how many top neurons we sample from. It starts from 1,\n",
    "        #because we choose the top neuron when possible, e.g. when we can find an ensemble\n",
    "        # that is larger than min ensemble size. If there is no ensemble with the top neuron\n",
    "        # we increase the number of neurons to sample from.\n",
    "        self.n_neurons_for_sampling=1\n",
    "        n=0\n",
    "        min_assembly_size=self.options_dict['min_assembly_size']\n",
    "        #index for switching between top neurons for fitting ensemble when the first neurons\n",
    "        #doesn't give large enough ensemble\n",
    "        index=-1\n",
    "        #A while loop for trying sampling other neurons if the found ensemble size is smaller\n",
    "        #than threshold.\n",
    "        while n<min_assembly_size:\n",
    "            start=time.time()\n",
    "            seed=self.repeated_seed(C,index)\n",
    "            end=time.time()\n",
    "            print('old seed', end-start,seed)\n",
    "            seed=self.fast_sort(C,index)\n",
    "            end2=time.time()\n",
    "            print('new seed',end2-end,seed)\n",
    "            n=1\n",
    "            current_v=X[seed,:]\n",
    "            current_v_unnorm=current_v.copy()\n",
    "            selected_neurons=np.zeros((X.shape[0]),dtype=bool)\n",
    "            #Seed current_v\n",
    "            selected_neurons[seed]=1\n",
    "            #Fake cost to initiate while loop\n",
    "            max_cost_delta=1000\n",
    "            C_summed_unnorm=0\n",
    "            max_delta_neuron=seed\n",
    "            while max_cost_delta>0:\n",
    "                #Add the x corresponding to the max delta neuron to C_sum. Saves computational\n",
    "                #time.\n",
    "                #print(n)\n",
    "                C_summed_unnorm=self.sum_C(C_summed_unnorm,C,max_delta_neuron)\n",
    "                C_summed=(1./n)*C_summed_unnorm\n",
    "                dot_squared=self.calculate_dot_squared(C_summed)\n",
    "                #invert the 0's and 1's in the array which stores which neurons have already\n",
    "                #been selected into the assembly to use it as a mask\n",
    "                masked_dot_squared=self.mask_dot_squared(selected_neurons,dot_squared)\n",
    "                max_delta_neuron=np.argmax(masked_dot_squared)\n",
    "                cost_delta=self.calculate_cost_delta(C_summed[max_delta_neuron],current_v)\n",
    "                if cost_delta>0:\n",
    "                    selected_neurons[max_delta_neuron]=1\n",
    "                    current_v_unnorm= self.sum_v(current_v_unnorm,max_delta_neuron,X)\n",
    "                    n+=1\n",
    "                    current_v=(1./n)*current_v_unnorm\n",
    "                max_cost_delta=cost_delta\n",
    "            index+=-1\n",
    "        print('nr of neurons in ensemble',n)\n",
    "        current_u=np.zeros((X.shape[0],1))\n",
    "        current_u[selected_neurons,0]=np.clip(C_summed[selected_neurons],a_min=0,a_max=None)/(current_v**2).sum()\n",
    "        self.U=np.concatenate((self.U,current_u),axis=1)\n",
    "        self.V=np.concatenate((self.V,current_v.reshape(1,self.sz[1])),axis=0)\n",
    "        return current_u, current_v, C, selected_neurons\n",
    "\n",
    "    def repeated_seed(self,C,index):\n",
    "        nr_neurons_to_av=self.options_dict['seed_neuron_av_nr']\n",
    "        sorted_similarities=np.sort(C,axis=1)[:,:-1][:,C.shape[0]-nr_neurons_to_av-1:]\n",
    "        average_similarities=np.mean(sorted_similarities,axis=1)\n",
    "        top_neurons=np.argsort(average_similarities)\n",
    "        seed=top_neurons[index]\n",
    "        return seed\n",
    "\n",
    "    def sorting_for_seed(self,C):\n",
    "        '''\n",
    "        This function sorts the similarity matrix C to find neurons that are most correlated\n",
    "        to their nr_neurons_to_av neighbors (we average over the neighbors).\n",
    "        '''\n",
    "        nr_neurons_to_av=self.options_dict['seed_neuron_av_nr']\n",
    "        sorted_similarities=np.sort(C,axis=1)[:,:-1][:,self.sz[0]-nr_neurons_to_av-1:]\n",
    "        average_similarities=np.mean(sorted_similarities,axis=1)\n",
    "        return seed\n",
    "    \n",
    "    def fast_sort(self,C,index):\n",
    "        nr_neurons_to_av=self.options_dict['seed_neuron_av_nr']\n",
    "        cutoff=self.sz[0]-nr_neurons_to_av-1\n",
    "        sorted_similarities_fast=np.partition(C,cutoff,axis=1)[:,cutoff:]\n",
    "        average_similarities_fast=(np.mean(sorted_similarities_fast,axis=1)*sorted_similarities_fast.shape[1]-np.max(sorted_similarities_fast,axis=1))/(sorted_similarities_fast.shape[1]-1)\n",
    "        top_neurons=np.argsort(average_similarities_fast)\n",
    "        seed=top_neurons[index]\n",
    "        #seed=np.argmax(average_similarities_fast)\n",
    "        return seed\n",
    "    \n",
    "    def update_C(self,X,C,u,v,selected_neurons):\n",
    "        #selected_neurons=np.nonzero(u)[0]\n",
    "        cross_term_init=X@(v.T)\n",
    "        cross_term=np.outer(u[selected_neurons],cross_term_init)\n",
    "        C[selected_neurons,:]=C[selected_neurons,:]-cross_term\n",
    "        ixgrid=np.ix_(~selected_neurons,selected_neurons)\n",
    "        C[ixgrid]=C[ixgrid]-cross_term.T[~selected_neurons,:]\n",
    "        return C\n",
    "\n",
    "    def fit_transform(self,X):\n",
    "        X=self.zscore(X)\n",
    "        self.sz=X.shape\n",
    "        self.U=np.zeros((X.shape[0],1))\n",
    "        self.V=np.zeros((1,X.shape[1]))\n",
    "        C=X@X.T\n",
    "        #np.fill_diagonal(C,0)\n",
    "        for iteration in range(0,self.n_ensembles):\n",
    "            current_u, current_v, C,selected_neurons=self.fit_one_ensemble(X,C)\n",
    "            U_V=current_u.reshape(self.sz[0],1)@current_v.reshape(1,self.sz[1])\n",
    "            X=X-U_V\n",
    "            C=X@X.T\n",
    "            C=self.update_C(X,C,current_u,current_v,selected_neurons)\n",
    "            #np.fill_diagonal(C,0)\n",
    "            print('ensemble nr', iteration)\n",
    "            cost=np.mean(X*X)\n",
    "            print('cost',cost)\n",
    "        #After fitting arrays discard the zero initialization rows and columns from U and V.\n",
    "        self.U=self.U[:,1:]\n",
    "        self.V=self.V[1:,:]\n",
    "        return self.U, self.V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/maria/Documents/EnsemblePursuit_old/experiments/natimg2800_M170717_MP034_2017-09-11.mat'\n",
    "data = io.loadmat(path)\n",
    "resp = data['stim'][0]['resp'][0]\n",
    "spont =data['stim'][0]['spont'][0]\n",
    "X=subtract_spont(spont,resp).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old seed 5.5862157344818115 1165\n",
      "new seed 0.8343744277954102 1165\n",
      "nr of neurons in ensemble 1432\n",
      "ensemble nr 0\n",
      "cost 0.9940062256834048\n",
      "old seed 5.260864019393921 9744\n",
      "new seed 0.8656191825866699 9744\n",
      "nr of neurons in ensemble 4981\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fb50ca81da5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptions_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'seed_neuron_av_nr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'min_assembly_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mep_np\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEnsemblePursuitNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_ensembles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mep_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a22bf60b1ddc>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mU_V\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mcurrent_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mU_V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m#np.fill_diagonal(C,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options_dict={'seed_neuron_av_nr':100,'min_assembly_size':8}\n",
    "ep_np=EnsemblePursuitNumpy(n_ensembles=3,lambd=0.01,options_dict=options_dict)\n",
    "U,V=ep_np.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_neurons_to_av=self.options_dict['seed_neuron_av_nr']\n",
    "        cutoff=self.sz[0]-nr_neurons_to_av-1\n",
    "        sorted_similarities_fast=np.partition(C,cutoff,axis=1)[:,cutoff:]\n",
    "        average_similarities_fast=(np.mean(sorted_similarities_fast,axis=1)*sorted_similarities_fast.shape[1]-np.max(sorted_similarities_fast,axis=1))/(sorted_similarities_fast.shape[1]-1)\n",
    "        top_neurons=np.argsort(average_similarities_fast)\n",
    "        seed=top_neurons[index]\n",
    "        #seed=np.argmax(average_similarities_fast)\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1165\n",
      "seed fast 1165\n",
      "nr of neurons in ensemble 1432\n",
      "ensemble nr 0\n",
      "cost 0.9940062256834048\n",
      "seed 1165\n",
      "seed fast 1165\n",
      "nr of neurons in ensemble 1432\n",
      "ensemble nr 1\n",
      "cost 14641046712.677303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-27d954a06abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptions_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'seed_neuron_av_nr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'min_assembly_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mep_np\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEnsemblePursuitNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_ensembles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mep_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-53e5f4065a4a>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m#np.fill_diagonal(C,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ensembles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mcurrent_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mU_V\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mcurrent_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mU_V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-53e5f4065a4a>\u001b[0m in \u001b[0;36mfit_one_ensemble\u001b[0;34m(self, X, C)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#than threshold.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmin_assembly_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeated_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-53e5f4065a4a>\u001b[0m in \u001b[0;36mrepeated_seed\u001b[0;34m(self, C, index)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepeated_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mnr_neurons_to_av\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed_neuron_av_nr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0msorted_similarities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnr_neurons_to_av\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0maverage_similarities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_similarities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mtop_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_similarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options_dict={'seed_neuron_av_nr':100,'min_assembly_size':8}\n",
    "ep_np=EnsemblePursuitNumpy(n_ensembles=150,lambd=0.01,options_dict=options_dict)\n",
    "U,V=ep_np.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "def zscore(X, axis=0):\n",
    "    mean_X= np.mean(X,axis=axis)\n",
    "    std_X = np.std(X, axis=axis) + 1e-10\n",
    "    X -= np.expand_dims(mean_X, axis)\n",
    "    X /= np.expand_dims(std_X, axis)\n",
    "\n",
    "    return X\n",
    "\n",
    "def test_train_split(data,stim):\n",
    "    unique, counts = np.unique(stim.flatten(), return_counts=True)\n",
    "    count_dict=dict(zip(unique, counts))\n",
    "\n",
    "    keys_with_enough_data=[]\n",
    "    for key in count_dict.keys():\n",
    "        if count_dict[key]==2:\n",
    "            keys_with_enough_data.append(key)\n",
    "\n",
    "    filtered_stims=np.isin(stim.flatten(),keys_with_enough_data)\n",
    "\n",
    "    #Arrange data so that responses with the same stimulus are adjacent\n",
    "    z=stim.flatten()[np.where(filtered_stims)[0]]\n",
    "    sortd=np.argsort(z)\n",
    "    istim=np.sort(z)\n",
    "    X=data[filtered_stims,:]\n",
    "    out=X[sortd,:].copy()\n",
    "\n",
    "    x_train=out[::2,:]\n",
    "    y_train=istim[::2]\n",
    "    x_test=out[1::2,:]\n",
    "    y_test=istim[1::2]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def stimulus_correlation(V, istim):\n",
    "    x_train,x_test,y_train,y_test= test_train_split(V,istim)\n",
    "    cc = np.mean(zscore(x_train) * zscore(x_test), axis=0)\n",
    "    return cc\n",
    "\n",
    "def evaluate_model(V,istim):\n",
    "    x_train,x_test,y_train,y_test= test_train_split(V,istim)\n",
    "\n",
    "    corr_mat = corr_matrix(x_train.T, x_test.T)\n",
    "    print(np.mean(np.argmax(corr_mat, axis=0) == np.arange(0,x_train.shape[0],1,int)))\n",
    "\n",
    "\n",
    "def corr_matrix(x,y):\n",
    "    '''\n",
    "    calculate correlation matrix\n",
    "    '''\n",
    "\n",
    "    x = x- np.mean(x,axis=0)\n",
    "    y = y- np.mean(y,axis=0)\n",
    "\n",
    "    x /= np.std(x,axis=0) + 1e-10\n",
    "    y /= np.std(y,axis=0) + 1e-10\n",
    "\n",
    "    c = x.T @ y\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def subtract_spont(spont,resp):\n",
    "    #print(spont)\n",
    "    mu = spont.mean(axis=0)\n",
    "    sd = spont.std(axis=0) + 1e-6\n",
    "    resp = (resp - mu) / sd\n",
    "    spont = (spont - mu) / sd\n",
    "    sv,u = eigsh(spont.T @ spont, k=32)\n",
    "    resp = resp - (resp @ u) @ u.T\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew\n",
    "import time\n",
    "from scipy.stats import zscore\n",
    "import sys\n",
    "\n",
    "def new_ensemble(X, C, seed_timecourse, lam, discard_first_neuron = False):\n",
    "    # X are the NT by NN neural activity traces (z-scored)\n",
    "    # C is the covariance matrix of X\n",
    "    # seed_timecourse initializes the ensemble\n",
    "    # lam is the explained variance threshold\n",
    "    # discard the first neuron in the ensemble (if this was used to seed the pursuit)\n",
    "\n",
    "    NT, NN = X.shape\n",
    "    mask_neurons=np.ones((NN,),dtype=bool)\n",
    "\n",
    "    # compute initial bias\n",
    "    bias = seed_timecourse @ X\n",
    "    current_v = seed_timecourse\n",
    "\n",
    "    # initialize C_summed\n",
    "    C_summed = bias.flatten()\n",
    "\n",
    "    # keep track of neuron order\n",
    "    iorder = np.zeros(NN, 'int32')\n",
    "\n",
    "    n = 0\n",
    "    while True:\n",
    "        # at each iteration, first determine the neuron to be added\n",
    "        imax = np.argmax(C_summed * mask_neurons)\n",
    "\n",
    "        # compute norm of ensemble trace\n",
    "        vnorm = np.sum(current_v**2)\n",
    "\n",
    "        # compute delta cost function\n",
    "        cost_delta = np.maximum(0., C_summed[imax])**2 / vnorm\n",
    "\n",
    "        # if cost/variance explained is less than lam (* n_timepoints) then break\n",
    "        if cost_delta<lam*NT:\n",
    "            break\n",
    "\n",
    "        # zero out freshly added neuron\n",
    "        mask_neurons[imax] = False\n",
    "\n",
    "        if n==0 and discard_first_neuron:\n",
    "            discard_first_neuron = False\n",
    "            continue\n",
    "\n",
    "        # add column of C\n",
    "        C_summed = C_summed + C[:, imax]\n",
    "\n",
    "        # add column of X\n",
    "        current_v = current_v + X[:, imax]\n",
    "\n",
    "        # keep track of neurons in ensembles\n",
    "        iorder[n] = imax\n",
    "\n",
    "        n = n+1\n",
    "\n",
    "    # take only first n neurons\n",
    "    iorder = iorder[:n]\n",
    "\n",
    "    return iorder, current_v\n",
    "\n",
    "def one_round_of_kmeans(V, X, lam=0.01, threshold=True):\n",
    "    # V are the NT by nK cluster activity traces\n",
    "    # X are the NT by NN neural activity traces (z-scored)\n",
    "    # if the threshold is true, neurons only make it into a cluster if their explained variance is above lam\n",
    "    # this is useful in the last stages when most neurons only have noise left\n",
    "\n",
    "    NT, nK = V.shape\n",
    "\n",
    "    # computes projections of neurons onto components\n",
    "    cc = V.T @ X\n",
    "\n",
    "    # take the biggest projection component for each neuron\n",
    "    imax = np.argmax(cc, axis=0)\n",
    "\n",
    "    # for every neuron, compute max projection\n",
    "    w = np.max(cc, axis=0)\n",
    "\n",
    "    # explained variance for each neuron\n",
    "    amax = np.maximum(0, w)**2/NT\n",
    "\n",
    "    # initialize total explained variance for each cluster\n",
    "    vm = np.zeros((nK,))\n",
    "\n",
    "    # update each cluster in k-means\n",
    "    for j in range(nK):\n",
    "        # take all neurons assigned to this cluster\n",
    "        ix = imax==j\n",
    "        if threshold:\n",
    "            ix = np.logical_and(ix, amax>lam)\n",
    "\n",
    "        # if there are more than 0 neurons assigned\n",
    "        if np.sum(ix)>0:\n",
    "            # update the component to be the mean  of assigned neurons\n",
    "            V[:,j] = X[:, ix] @ w[ix]\n",
    "\n",
    "            # compute total explained variance for this cluster\n",
    "            vm[j] = np.sum(amax[ix])\n",
    "\n",
    "    # re-normalize each column of V separately\n",
    "    V = V/np.sum(V**2 + 1e-6, axis=0)**.5\n",
    "\n",
    "    return V, vm\n",
    "\n",
    "def one_round_of_PCA(V, C):\n",
    "    # computes projections of neurons onto components\n",
    "    for t in range(5):\n",
    "        V = C @ V\n",
    "        V /= np.sum(V**2)**.5\n",
    "        V = V.flatten()\n",
    "\n",
    "    return V\n",
    "\n",
    "def initialize_kmeans(X, nK, lam):\n",
    "    # initialize k-means for matrix X (NT by NN)\n",
    "    # the columns of X should be Z-SCORED\n",
    "\n",
    "    # initialize k-means centers. THINK HOW TO MAKE THIS DETERMINISTIC\n",
    "    t0 = time.time()\n",
    "    model = PCA(n_components=nK, random_state = 101).fit(X.T)\n",
    "    V = model.components_.T\n",
    "    V = V * np.sign(skew(V.T @ X,axis=1))\n",
    "    print('obtained %d PCs in %2.4f seconds'%(nK, time.time()-t0))\n",
    "\n",
    "    #np.random.seed(101)\n",
    "    #rperm = np.random.permutation(X.shape[1])\n",
    "    #V = X[:, rperm[:nK]]\n",
    "    #V = np.random.randn(X.shape[0], nK)\n",
    "\n",
    "    # keep V as unit norm vectors\n",
    "    V /= np.sum(V**2 + 1e-6, axis=0)**.5\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 10 iterations is plenty\n",
    "    for j in range(10):\n",
    "        # run one round of k-means and update the cluster activities (V)\n",
    "        V, vm = one_round_of_kmeans(V, X, lam, j>5)\n",
    "\n",
    "    print('initialized %d clusters with k-means in %2.4f seconds'%(nK, time.time()-t0))\n",
    "\n",
    "    return V, vm\n",
    "\n",
    "def initialize_sort(C):\n",
    "    nr_neurons_to_av=100\n",
    "    cutoff=C.shape[0]-nr_neurons_to_av-1\n",
    "    sorted_similarities_fast=np.partition(C,cutoff,axis=1)[:,cutoff:]\n",
    "    average_similarities_fast=(np.mean(sorted_similarities_fast,axis=1)*sorted_similarities_fast.shape[1]-np.max(sorted_similarities_fast,axis=1))/(sorted_similarities_fast.shape[1]-1)\n",
    "    #top_neurons=np.argsort(average_similarities_fast)\n",
    "    #seed=top_neurons[index]\n",
    "    seed=np.argmax(average_similarities_fast)\n",
    "    return seed\n",
    "\n",
    "class EnsemblePursuit():\n",
    "    def __init__(self,n_components= 100,lam=0.01, n_kmeans = 25):\n",
    "        self.n_components=n_components\n",
    "        self.lam=lam\n",
    "        self.n_kmeans=n_kmeans\n",
    "\n",
    "    def fit(self,X):\n",
    "        nK=self.n_components\n",
    "        lam=self.lam\n",
    "        n_kmeans = self.n_kmeans\n",
    "\n",
    "        NT, NN = X.shape\n",
    "\n",
    "        # z-score along time dimension\n",
    "        X = zscore(X, axis=0)\n",
    "\n",
    "        # convert to float64 for numerical precision\n",
    "        X = np.float64(X)\n",
    "\n",
    "        # initialize k-means clusters and compute their variance in vm\n",
    "        V, vm = initialize_kmeans(X, n_kmeans, lam)\n",
    "\n",
    "        # initialize vectors in ensemble pursuit (Vs)\n",
    "        vs = np.zeros((NT, nK))\n",
    "\n",
    "        # initialize U\n",
    "        U = np.zeros((NN, nK))\n",
    "\n",
    "        # precompute covariance matrix of neurons\n",
    "        C = X.T @ X\n",
    "\n",
    "        # keep track of number of neurons per ensemble\n",
    "        ns = np.zeros(nK,)\n",
    "\n",
    "        # time the ensemble pursuit\n",
    "        t0 = time.time()\n",
    "\n",
    "        # keep track of neuron order in ensembles\n",
    "\n",
    "        self.order = []\n",
    "        self.seed = np.zeros((NT, nK))\n",
    "\n",
    "        #  outer loop\n",
    "        for j in range(nK):\n",
    "            # initialize with \"biggest\" k-means ensemble (by variance)\n",
    "            imax = np.argmax(vm)\n",
    "\n",
    "            # zscore the seed trace\n",
    "            seed = zscore(V[:, imax])\n",
    "            seed=initialize_sort(C)\n",
    "\n",
    "            # fit one ensemble starting from this seed\n",
    "            iorder, current_v  = new_ensemble(X, C, seed, lam)\n",
    "            self.order.append(iorder)\n",
    "            self.seed[:, j] = seed\n",
    "\n",
    "            # keep track of number of neurons\n",
    "            ns[j] = len(iorder)\n",
    "\n",
    "            # normalize current_v to unit norm\n",
    "            current_v /= np.sum(current_v**2)**.5\n",
    "\n",
    "            # update column of Vs\n",
    "            vs[:,j] = current_v\n",
    "\n",
    "            # projection of each neuron onto this ensemble trace\n",
    "            w = current_v @ X\n",
    "\n",
    "            # update weights for neurons in this ensemble\n",
    "            U[iorder, j] = w[iorder]\n",
    "\n",
    "            # update activity trace\n",
    "            X[:, iorder] -= np.outer(current_v, w[iorder])\n",
    "\n",
    "            # rank one update to C using wtw\n",
    "            wtw = np.outer(w[iorder],  w)\n",
    "\n",
    "            # update the columns\n",
    "            C[:, iorder] -= wtw.T\n",
    "\n",
    "            # update the rows\n",
    "            C[iorder, :] -= wtw\n",
    "\n",
    "            # add back term for the submatrix of neurons in this ensemble\n",
    "            C[iorder[:, np.newaxis], iorder] += wtw[:, iorder]\n",
    "\n",
    "            # run one round of k-means because we changed X\n",
    "            #V, vm = one_round_of_kmeans(V, X, lam)\n",
    "\n",
    "            if j%25==0 or j == nK-1:\n",
    "                print('ensemble %d, time %2.2f, nr neurons %d, EV %2.4f'%(j, time.time() - t0, len(iorder), 1-np.mean(X**2)))\n",
    "        print('average sparsity is %2.4f'%(np.mean(U>1e-5)))\n",
    "\n",
    "        self.components_ = vs\n",
    "        self.weights = U\n",
    "        #self.residual_kmeans = V\n",
    "\n",
    "        # the fit function has to return the model\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained 150 PCs in 3.7254 seconds\n",
      "initialized 150 clusters with k-means in 10.2868 seconds\n",
      "ensemble 0, time 2.65, nr neurons 1432, EV 0.0058\n",
      "ensemble 25, time 40.26, nr neurons 253, EV 0.0466\n",
      "ensemble 50, time 66.43, nr neurons 72, EV 0.0638\n",
      "ensemble 75, time 88.76, nr neurons 70, EV 0.0752\n",
      "ensemble 100, time 108.50, nr neurons 37, EV 0.0847\n",
      "ensemble 125, time 126.65, nr neurons 90, EV 0.0926\n",
      "ensemble 149, time 142.21, nr neurons 44, EV 0.0992\n",
      "average sparsity is 0.0184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EnsemblePursuit at 0x7f909418da90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep=EnsemblePursuit(n_components=150,lam=0.01,n_kmeans=150)\n",
    "ep.fit(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained 150 PCs in 3.5741 seconds\n",
      "initialized 150 clusters with k-means in 9.6340 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4cf68d089c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mep_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEnsemblePursuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_kmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mep_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-33807b5d4af9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# fit one ensemble starting from this seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0miorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_v\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnew_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miorder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-33807b5d4af9>\u001b[0m in \u001b[0;36mnew_ensemble\u001b[0;34m(X, C, seed_timecourse, lam, discard_first_neuron)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# compute initial bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_timecourse\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mcurrent_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_timecourse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "ep_=EnsemblePursuit(n_components=150,lam=0.01,n_kmeans=150)\n",
    "ep_.fit(X.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
